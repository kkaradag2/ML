{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyjbd2MIkzBy6uSm/xJCGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkaradag2/ML/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Gerçek dünyadaki birçok problemde, bir değişkenin başka bir değişkenle olan ilişkisini anlamak isteriz. Örneğin bir ürünün reklam bütçesi arttıkça satış miktarının nasıl değiştiği, bir öğrencinin ders çalışma süresi ile aldığı not arasındaki ilişki ya da bir evin oda sayısı arttıkça fiyatının nasıl değiştiği gibi. Bu tip problemlerde amaç, elimizdeki verilere bakarak bu ilişkiyi mümkün olduğunca basit ama anlamlı bir şekilde modellemektir.\n",
        "\n",
        "Linear regression bu amaçla kullanılan en temel yöntemlerden biridir. Temel amacı : bağımlı değişken $y$, bağımsız değişken $x$ ile doğrusal bir ilişkiye sahiptir. Bu ilişki matematiksel olarak şu şekilde ifade edilir:\n",
        "\n",
        "$$\n",
        "f(x) = wx + b\n",
        "$$\n",
        "\n",
        "Burada:\n",
        "\n",
        "- $x$ bağımsız değişken. Örneğin, bir evin oda sayısı yada öğrencinin ders çalışma süresi\n",
        "- $y$ bağımlı değişken. Örneğin, bir evin fiyatı, öğrencinin sınavda aldığı not\n",
        "- $w$ eğimi temsil eder. Yani $x$ bir birim arttığında $y$’nin ne kadar değişeceğini söyler.\n",
        "- $b$ ise **bias** (kesim noktası) olarak adlandırılır ve doğrunun $y$-eksenini kestiği noktayı ifade eder.\n",
        "\n",
        "Bu fonksiyon aynı zamanda modelimizin tahmin fonksiyonudur\n",
        "\n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "\n",
        "şeklinde yazılır. Amaç, elimizdeki veriler için en uygun $w$ ve $b$ değerlerini bulmaktır.\n",
        "\n",
        "![Linear Regression Grafikleri](https://raw.githubusercontent.com/kkaradag2/ML/refs/heads/main/assets/linear_regression_graf.png)\n",
        "\n",
        "Grafik olarak bakıldığında, elimizde bazı veri noktaları vardır ve biz bu noktalara en iyi uyan doğruyu çizmeye çalışırız. Böylece bize bilmediğimiz yeni bir bilgi geldiğinde (örneğin bir binanın oda sayıs) bu veriyi tahmin modelimize koyarak bir tahnin yapmaya çalışırız.\n",
        "\n",
        "Grafikte, $w$ doğrunun eğimini belirlerken, $b$ doğrunun yukarı veya aşağı kaymasını sağlar. Eğer $b=0$ olsaydı doğru orijinden geçmek zorunda kalırdı; fakat gerçek hayatta çoğu problemde u durum mantıklı değildir. (Bir evin bedava olamayacağı gibi). Bu yüzden bias terimi modele esneklik kazandırır, bir başlangıç değeri oluşturur.\n",
        "\n",
        "\n",
        "## Hata (Error) ve Loss (Kayıp) Fonksiyonu\n",
        "\n",
        "Modelin ne kadar iyi olduğunu anlayabilmek için tahmin edilen değer ile gerçek değer arasındaki farkı ölçmemiz gerekir. Bu farka hata (**error**) denir:\n",
        "\n",
        "$$\n",
        "\\text{hata} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "Ancak tek bir veri noktası için hataya bakmak yeterli değildir. Birden fazla veri noktası olduğunda tüm hataları tek bir sayı ile ifade etmek isteriz. Bunun için en sık kullanılan yöntem, hataların karesinin ortalamasını almaktır. Bu da loss (kayıp) fonksiyonudur:\n",
        "\n",
        "$$\n",
        "L(w,b) = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "Kare alınmasının nedeni, hataların pozitif veya negatif olmasına bakmaksızın büyüklüklerini ölçebilmek ve türev almayı matematiksel olarak kolaylaştırmaktır.\n",
        "\n",
        "---\n",
        "\n",
        "## $w$ ve $b$ Nasıl Bulunur? Gradient Descent\n",
        "\n",
        "Şimdi asıl kritik noktaya geliyoruz: $w$ ve $b$’yi nasıl bulacağız?\n",
        "\n",
        "Amaç, bu loss fonksiyonunu **minimum yapan** $w$ ve $b$ değerlerini bulmaktır. Bunun için **gradient descent** yöntemi kullanılır.\n",
        "\n",
        "Gradient descent’in temel fikri şudur: Eğer $w$’yi çok az değiştirirsek loss artıyor mu azalıyor mu? Aynı soru $b$ için de sorulur. Bu sorunun cevabını **türev** verir. Bu yüzden loss fonksiyonunun $w$ ve $b$’ye göre türevini alırız:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}, \\quad \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "Türevlerin işareti bize hangi yönde ilerlememiz gerektiğini söyler:\n",
        "\n",
        "- Türev **pozitifse** parametreyi azaltırız.\n",
        "- Türev **negatifse** parametreyi artırırız.\n",
        "\n",
        "Güncelleme kuralları:\n",
        "\n",
        "$$\n",
        "w = w - \\alpha \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "Buradaki $\\alpha$ öğrenme oranıdır (**learning rate**). Adımın ne kadar büyük atılacağını belirler.\n",
        "\n",
        "---\n",
        "\n",
        "## Sayısal Örnek (Tek Veri Noktası)\n",
        "\n",
        "Basit olması açısından tek bir veri noktası ele alalım:\n",
        "\n",
        "$$\n",
        "(x,y) = (2,5)\n",
        "$$\n",
        "\n",
        "Modelimiz:\n",
        "\n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "\n",
        "Bu veri noktası için hata:\n",
        "\n",
        "$$\n",
        "\\hat{y} - y = (2w + b - 5)\n",
        "$$\n",
        "\n",
        "Loss fonksiyonu:\n",
        "\n",
        "$$\n",
        "L(w,b) = (2w + b - 5)^2\n",
        "$$\n",
        "\n",
        "Şimdi türevleri alalım. Burada zincir kuralı kullanılır. Genel olarak:\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}(u^2) = 2u \\cdot \\frac{du}{dx}\n",
        "$$\n",
        "\n",
        "### $w$’ye göre türev\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}\n",
        "= 2(2w+b-5)\\cdot 2\n",
        "= 4(2w+b-5)\n",
        "$$\n",
        "\n",
        "### $b$’ye göre türev\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b}\n",
        "= 2(2w+b-5)\n",
        "$$\n",
        "\n",
        "Başlangıç değerlerini:\n",
        "\n",
        "$$\n",
        "w_0 = 0, \\quad b_0 = 0\n",
        "$$\n",
        "\n",
        "olarak seçelim.\n",
        "\n",
        "Bu noktadaki türevler:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = 4(-5) = -20\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = 2(-5) = -10\n",
        "$$\n",
        "\n",
        "Öğrenme oranını:\n",
        "\n",
        "$$\n",
        "\\alpha = 0.1\n",
        "$$\n",
        "\n",
        "seçelim.\n",
        "\n",
        "Güncelleme sonrası:\n",
        "\n",
        "$$\n",
        "w_1 = 0 - 0.1(-20) = 2\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_1 = 0 - 0.1(-10) = 1\n",
        "$$\n",
        "\n",
        "Yeni parametrelerle tahmin:\n",
        "\n",
        "$$\n",
        "\\hat{y} = 2\\cdot 2 + 1 = 5\n",
        "$$\n",
        "\n",
        "Gerçek değerle aynı olduğu için hata sıfırdır ve loss da sıfır olur.\n",
        "\n",
        "Bu örnekte tek adımda doğru sonuca ulaşılmasının nedeni, yalnızca tek bir veri noktası olması ve problemin tam belirlenmiş olmasıdır. Genel durumda, özellikle çok sayıda veri noktası varken gradient descent birçok adımda yavaş yavaş minimuma yaklaşır.\n",
        "\n",
        "---\n",
        "\n",
        "## Üç Veri Noktalı Örnek\n",
        "\n",
        "Şimdi tek bir veri noktası yerine üç veri noktası içeren daha gerçekçi bir örnek ele alalım. Veri setimiz şu olsun:\n",
        "\n",
        "$$\n",
        "(x,y) = \\{(1,2), (2,3), (3,5)\\}\n",
        "$$\n",
        "\n",
        "Modelimiz yine aynıdır:\n",
        "\n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "\n",
        "Ama artık üç farklı tahmin ve üç farklı hata vardır.\n",
        "\n",
        "### Tahminler\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = w(1) + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y}_2 = w(2) + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y}_3 = w(3) + b\n",
        "$$\n",
        "\n",
        "### Hatalar\n",
        "\n",
        "$$\n",
        "e_1 = (w+b-2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "e_2 = (2w+b-3)\n",
        "$$\n",
        "\n",
        "$$\n",
        "e_3 = (3w+b-5)\n",
        "$$\n",
        "\n",
        "Loss fonksiyonu (kare hataların ortalaması):\n",
        "\n",
        "$$\n",
        "L(w,b)=\\frac{1}{3}\\Big[(w+b-2)^2 + (2w+b-3)^2 + (3w+b-5)^2\\Big]\n",
        "$$\n",
        "\n",
        "Amaç, bu fonksiyonu minimum yapan $w$ ve $b$ değerlerini bulmaktır.\n",
        "\n",
        "---\n",
        "\n",
        "## Türevlerin Alınması\n",
        "\n",
        "### $w$’ye göre türev\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}\n",
        "=\n",
        "\\frac{1}{3}\\Big[\n",
        "2(w+b-2)\\cdot 1\n",
        "+ 2(2w+b-3)\\cdot 2\n",
        "+ 2(3w+b-5)\\cdot 3\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "Açarsak:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}\n",
        "=\n",
        "\\frac{2}{3}\\Big[\n",
        "(w+b-2) + 2(2w+b-3) + 3(3w+b-5)\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "### $b$’ye göre türev\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b}\n",
        "=\n",
        "\\frac{1}{3}\\Big[\n",
        "2(w+b-2) + 2(2w+b-3) + 2(3w+b-5)\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Sayısal Güncelleme (Başlangıç Noktası)\n",
        "\n",
        "Başlangıç değerlerini:\n",
        "\n",
        "$$\n",
        "w_0 = 0, \\quad b_0 = 0\n",
        "$$\n",
        "\n",
        "olarak seçelim.\n",
        "\n",
        "Bu durumda hatalar:\n",
        "\n",
        "$$\n",
        "e_1 = -2,\\quad e_2 = -3,\\quad e_3 = -5\n",
        "$$\n",
        "\n",
        "Türevleri yerine koyalım.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}\n",
        "=\n",
        "\\frac{2}{3}\\big[(-2)+2(-3)+3(-5)\\big]\n",
        "=\n",
        "\\frac{2}{3}(-23)\n",
        "=\n",
        "-\\frac{46}{3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b}\n",
        "=\n",
        "\\frac{2}{3}\\big[(-2)+(-3)+(-5)\\big]\n",
        "=\n",
        "\\frac{2}{3}(-10)\n",
        "=\n",
        "-\\frac{20}{3}\n",
        "$$\n",
        "\n",
        "Öğrenme oranını yine:\n",
        "\n",
        "$$\n",
        "\\alpha = 0.1\n",
        "$$\n",
        "\n",
        "olarak alalım.\n",
        "\n",
        "### Parametre Güncellemesi\n",
        "\n",
        "$$\n",
        "w_1 = 0 - 0.1\\left(-\\frac{46}{3}\\right)\n",
        "= \\frac{4.6}{3}\n",
        "\\approx 1.53\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_1 = 0 - 0.1\\left(-\\frac{20}{3}\\right)\n",
        "= \\frac{2}{3}\n",
        "\\approx 0.67\n",
        "$$\n",
        "\n",
        "Bu noktada model artık:\n",
        "\n",
        "$$\n",
        "\\hat{y} = 1.53x + 0.67\n",
        "$$\n",
        "\n",
        "şeklindedir.\n",
        "\n",
        "Bu doğru, veri noktalarının hiçbirinden tam olarak geçmez ama hepsine ortalama anlamda daha yakındır. Gradient descent’in amacı da budur: tek bir noktayı değil, tüm veriyi birlikte en iyi açıklayan doğruyu bulmak.\n",
        "\n",
        "Bu örnekte tek adımda sıfır hata elde edilmez. Çünkü artık problem tam belirlenmiş değildir ve model, farklı noktalar arasındaki dengeyi kurmak zorundadır. Gradient descent bu dengeyi her adımda biraz daha iyileştirerek kurar.\n",
        "\n",
        "Varsayımlar:\n",
        "\n",
        "- Veri: $(1,2),(2,3),(3,5)$\n",
        "- Loss: MSE\n",
        "- Öğrenme oranı: $\\alpha = 0.1$\n",
        "- Başlangıç: $w_0=0,; b_0=0$\n",
        "\n",
        "## İterasyonlara Göre Değişim\n",
        "\n",
        "| İterasyon | $w$ | $b$ | $\\frac{\\partial L}{\\partial w}$ | $\\frac{\\partial L}{\\partial b}$ | Açıklama |\n",
        "|----------|-----|-----|----------------------------------|----------------------------------|----------|\n",
        "| 0  | $0.00$ | $0.00$ | $-\\frac{46}{3}\\approx -15.33$ | $-\\frac{20}{3}\\approx -6.67$ | Başlangıç noktası |\n",
        "| 1 | $1.53$ | $0.67$ | $\\approx -3.78$ | $\\approx -1.56$ | Hata azaldı, gradyan küçüldü |\n",
        "| 2 | $1.91$ | $0.82$ | $\\approx -0.93$ | $\\approx -0.38$ | Minimuma yaklaşma |\n",
        "\n"
      ],
      "metadata": {
        "id": "uCE2SnE6wM4x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbIdUQXUzE1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}